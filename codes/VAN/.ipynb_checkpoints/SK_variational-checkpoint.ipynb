{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational methods for Sherrington-Kirkpatrick model\n",
    "Pan Zhang\n",
    "Institute of Theoretical Physics, Chinese Academy of Sciences\n",
    "\n",
    "We compuare variational free energy given by the variational mean-field method and the Variational Autoregressive Network (Physical Review Letters 122, 080602), for the Sherrington-Kirkpatrick spin glass model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SK model\n",
    "$n$ denotes number of variables, $J_{ij}=\\frac{1}{\\sqrt{n}}\\mathcal N(0,1)$ is the couplings matrix and $\\beta$ is the inverse temperature. One needs to notice that diagonal terms of the couplings matrix must be zero, i.e.\n",
    "$J_{ii}=0$, and the coupling matrix $\\mathbf J$ is symmetic.\n",
    "\n",
    "The following code generates a small instance of the SK model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch,math\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from scipy.special import logsumexp\n",
    "import sys\n",
    "device=torch.device('cpu')\n",
    "device=torch.device('cuda:0')\n",
    "n=20 # number of spins\n",
    "beta=0.5 # inverse temperature\n",
    "seed=1\n",
    "torch.manual_seed(seed)\n",
    "J=torch.randn(n,n,device=device)/math.sqrt(n)\n",
    "J = torch.triu(J,diagonal=1) # take the upper triangular matrix\n",
    "J = J+J.t() # make the coupling matrix symmetric\n",
    "J_np = J.cpu().numpy()\n",
    "J.requires_grad=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact enumerations for small systems\n",
    "Whent the number of spins is small, e.g. $n\\leq 20$, we can compute exactly the free energy, energy and entropy by enumerating all $2^n$ configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cfg_id_to_sample(cfg_id):\n",
    "    return np.array( [((cfg_id >> i) & 1) * 2 - 1 for i in range(n-1,-1,-1)])\n",
    "\n",
    "def list_energy(print_step=float('inf')):\n",
    "    samples=[]\n",
    "    energy_arr = []\n",
    "    for cfg_id in range(1 << n):\n",
    "        if (cfg_id + 1) % print_step == 0:\n",
    "            sys.stdout.write(\"\\rEnumerating all configurations: %d / 100\"%(int(cfg_id /(1 << n)*100)+1))\n",
    "        sample = cfg_id_to_sample(cfg_id)\n",
    "        energy_arr.append(sample.dot(J_np).dot(sample)/2.0)\n",
    "        samples.append(sample)\n",
    "        \n",
    "    cfg_id_arr = np.arange(1 << (n), dtype=int)\n",
    "    energy_arr = np.array(energy_arr)\n",
    "    energy_arr *= -1.0\n",
    "    samples = np.array(samples)\n",
    "\n",
    "    return cfg_id_arr, energy_arr\n",
    "    \n",
    "def f_exact():\n",
    "    if(n>20):\n",
    "        return 0,0,0\n",
    "    step=int((1<<n)/10.)\n",
    "    arr, energy_arr = list_energy(step)\n",
    "    logz = logsumexp(-beta * energy_arr)\n",
    "    f=-1.0*logz/beta/n\n",
    "    prob_arr=np.exp(-1.0*beta*energy_arr-logz)\n",
    "    E=np.sum(prob_arr*energy_arr)/n\n",
    "    S=beta*E+logz/n\n",
    "    print(\"\\nExact:\\tf=%.6f\\te=%.6f\\ts=%.6f\"%(f,E,S))\n",
    "    return f,E,S\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Mean-field\n",
    "The joint distribution of variables $x\\in\\{+1,-1\\}^n$ are factorized $P(x)=\\prod_ip(x_i).$\n",
    "\n",
    "Defining magnetization as $$m_i=\\sum_{x_i=\\{1,-1\\}}p(x_i)=p(x_i=1)-p(x_i=-1)$$\n",
    "\n",
    "Then the entropy of the factorized distribution is $$S=-\\sum_i\\sum_{x_i}p(x_i)\\log(p(x_i))=-\\sum_i\\left( \\frac{1+m_i}{2}\\log \\frac{1+m_i}{2}+ \\frac{1-m_i}{2}\\log \\frac{1-m_i}{2} \\right).$$.\n",
    "\n",
    "The average energy under the factorized distribution is written as $$\\langle E \\rangle=\\sum_{(ij)}\\langle E_{ij}(x_i,x_j)\\rangle_{x\\sim p(x)}=\\frac{1}{2}\\sum_{(ij)}\\sum_{x_i}\\sum_{x_j}E_{ij}(x_i,x_j)p(x_i)p(x_j)=-\\frac{1}{2}J_{ij}m_im_j.$$\n",
    "\n",
    "Using above expressions for average energy and free energy, the variational free energy is written as\n",
    "$$f=\\langle E \\rangle-\\frac{1}{\\beta}S.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enumerating all configurations: 100 / 100\n",
      "Exact:\tf=-1.507863\te=-0.244720\ts=0.631572\n",
      "Naive Mean-Field:\tf=-1.386294\te=-0.000000\ts=0.693147\tdiff=0.000\n"
     ]
    }
   ],
   "source": [
    "def get_entropy_fact(m):\n",
    "    \"\"\" get_h(m) returns entropy of a factoriz distribution of len(m) boolean variables.'\n",
    "m is the magnetization, i.e. (1+m)/2 = p(+1) and (1-m)/2 = p(-1)\"\"\"\n",
    "    return -1.0*torch.sum( (1+m)/2*torch.log((1+m)/2)+(1-m)/2*torch.log((1-m)/2) )\n",
    "\n",
    "def get_free_energy_fact(m):\n",
    "    entropy=get_entropy_fact(m)/n\n",
    "    energy=-0.5*m.t()@J@m/n\n",
    "    free_energy=energy-entropy/beta\n",
    "    return [free_energy,energy,entropy]\n",
    "\n",
    "def nmf():\n",
    "    damping=0.9\n",
    "    torch.manual_seed(seed)\n",
    "    diff=100;\n",
    "    max_iter=8000;\n",
    "    conv_crit = 1.0e-8\n",
    "    m=0\n",
    "    m_old=torch.tanh(torch.randn(n,1,device=device)) # magnetization, randomly initialized between [-1,1]\n",
    "    for i in range(max_iter):\n",
    "        max_iter=i+0\n",
    "        m=damping*m+(1-damping)*torch.tanh(beta*J@m_old)\n",
    "        diff=torch.norm(m-m_old)\n",
    "        if(diff<conv_crit):\n",
    "            break\n",
    "        m_old=m.clone()\n",
    "        m_old=m_old.clone()\n",
    "    [f_mf,e_mf,s_mf]=get_free_energy_fact(m)\n",
    "    print(\"Naive Mean-Field:\\tf=%.6f\\te=%.6f\\ts=%.6f\\tdiff=%.3f\"%(f_mf,e_mf,s_mf,diff))\n",
    "    return f_mf,e_mf,s_mf,diff\n",
    "\n",
    "f,E,S=f_exact()\n",
    "f_nmf,E_nmf,s_nmf,err_nmf=nmf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational autoregressive network\n",
    "In VAN, the variational distribution is the product of conditional distributions\n",
    "\n",
    "$p(\\mathbf x)=\\prod_ip(x_i|\\mathbf x_{j<i})$\n",
    "\n",
    "Here we used a very simple version of the VAN that \n",
    "\n",
    "$p(\\mathbf x)=\\prod_i\\left[\\hat x_i\\delta(x_i-1)+(1-\\hat x_i)\\delta(x_i+1)\\right]$ \n",
    "\n",
    "with \n",
    "\n",
    "$\\mathbf {\\hat x}=\\mathrm{Sigmoid}(\\mathbf W\\mathbf x)$,\n",
    "\n",
    "and $W\\in\\mathbf {n\\times n}$ is a lower-triangular matrix with diagonal terms being $0$. The REINFORCE (Williams 1992) algorithms with a simple baseline is used for computing gradients of variational free energy with respect to parameters $\\mathbf W$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0\t free energy=-0.904866 std=0.25451 energy=-0.0622 entropy=0.421315\n",
      "#100\t free energy=-1.31669 std=0.127507 energy=-0.326 entropy=0.49556\n",
      "#200\t free energy=-1.48864 std=0.0530314 energy=-0.265 entropy=0.611913\n",
      "#300\t free energy=-1.50781 std=0.00295863 energy=-0.245 entropy=0.631463\n",
      "#400\t free energy=-1.50784 std=0.00185979 energy=-0.243 entropy=0.632562\n",
      "#500\t free energy=-1.50791 std=0.00185464 energy=-0.242 entropy=0.632705\n",
      "#600\t free energy=-1.50784 std=0.0018542 energy=-0.244 entropy=0.632042\n",
      "#700\t free energy=-1.50785 std=0.00187904 energy=-0.244 entropy=0.632098\n",
      "#800\t free energy=-1.50782 std=0.00182032 energy=-0.244 entropy=0.631871\n",
      "#900\t free energy=-1.50783 std=0.00188075 energy=-0.242 entropy=0.632923\n"
     ]
    }
   ],
   "source": [
    "mask=torch.ones([n, n],device=device)\n",
    "mask = 1 - torch.triu(mask).t() # upper triangular mask\n",
    "mask.requires_grad=False\n",
    "W=torch.randn(n,n,device=device)\n",
    "W.requires_grad=True\n",
    "optimizer = torch.optim.Adam([W], lr=1e-2)\n",
    "batch_size=5000\n",
    "epsilon=1e-7\n",
    "print_steps=100\n",
    "\n",
    "def do_sample(batch_size):\n",
    "    samples = torch.zeros([batch_size, n],device=device)\n",
    "    for i in range(n):\n",
    "        x_hat = torch.sigmoid(samples@W)\n",
    "        samples[:, i] = torch.bernoulli(x_hat[:, i]) * 2 - 1\n",
    "    return samples,x_hat\n",
    "\n",
    "\n",
    "for step in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    W.data =W.data*mask\n",
    "    with torch.no_grad():\n",
    "        samples, x_hat = do_sample(batch_size)\n",
    "    x_hat = torch.sigmoid(samples@W)\n",
    "    m = (samples + 1) / 2\n",
    "    log_prob = (torch.log(x_hat + epsilon) * m + torch.log(1 - x_hat + epsilon) * (1 - m)).view(batch_size, -1).sum(dim=1)\n",
    "    with torch.no_grad():\n",
    "        energy = -0.5*torch.sum((samples@J)*samples,dim=1)\n",
    "        loss = log_prob + beta * energy\n",
    "    assert not energy.requires_grad\n",
    "    assert not loss.requires_grad\n",
    "    loss_reinforce = torch.mean((loss - loss.mean()) * log_prob)\n",
    "    loss_reinforce.backward()\n",
    "    optimizer.step()\n",
    "    if(step % print_steps == 0):\n",
    "        free_energy_mean = loss.mean() / beta / n\n",
    "        free_energy_std = loss.std() / beta / n\n",
    "        entropy_mean = -log_prob.mean() / n\n",
    "        energy_mean = energy.mean() / n\n",
    "        print(\"#%d\\t free energy=%.6g std=%.6g energy=%.3g entropy=%.6g\"%(step,free_energy_mean,free_energy_std,energy_mean,entropy_mean))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
