{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "batch_size=1000\n",
    "test_batch_size=1000\n",
    "epochs=1000\n",
    "device=torch.device(\"cpu\")\n",
    "device=torch.device(\"cuda:0\")\n",
    "data_path='../../../data'\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if device != torch.device('cpu') else {}\n",
    "train_loader = torch.utils.data.DataLoader( \n",
    "    datasets.MNIST(data_path, train=True,download=True, \n",
    "    transform=transforms.Compose([ transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=batch_size,\n",
    "    shuffle=False, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(datasets.MNIST(data_path, train=False,\n",
    "    transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,),\n",
    "    (0.3081,)) ])), batch_size=test_batch_size, shuffle=False, **kwargs)\n",
    "train_data=[]\n",
    "test_data=[]\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    train_data.append((data,target))\n",
    "for batch_idx, (data, target) in enumerate(test_loader):\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    test_data.append((data,target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#10 loss=0.533 test_loss=0.594 accuracy=0.851 time=0.419\n",
      "#20 loss=0.466 test_loss=0.483 accuracy=0.837 time=0.421\n",
      "#30 loss=0.213 test_loss=0.250 accuracy=0.933 time=0.419\n",
      "#40 loss=0.213 test_loss=0.244 accuracy=0.928 time=0.422\n",
      "#50 loss=0.179 test_loss=0.200 accuracy=0.943 time=0.419\n",
      "#60 loss=0.141 test_loss=0.154 accuracy=0.956 time=0.419\n",
      "#70 loss=0.186 test_loss=0.195 accuracy=0.942 time=0.420\n",
      "#80 loss=0.123 test_loss=0.125 accuracy=0.965 time=0.420\n",
      "#90 loss=0.139 test_loss=0.136 accuracy=0.960 time=0.422\n",
      "#100 loss=0.109 test_loss=0.108 accuracy=0.970 time=0.420\n",
      "#110 loss=0.105 test_loss=0.098 accuracy=0.972 time=0.419\n",
      "#120 loss=0.228 test_loss=0.269 accuracy=0.907 time=0.419\n",
      "#130 loss=0.102 test_loss=0.088 accuracy=0.975 time=0.420\n",
      "#140 loss=0.098 test_loss=0.083 accuracy=0.976 time=0.419\n",
      "#150 loss=0.246 test_loss=0.209 accuracy=0.936 time=0.420\n",
      "#160 loss=0.095 test_loss=0.077 accuracy=0.978 time=0.421\n",
      "#170 loss=0.092 test_loss=0.073 accuracy=0.978 time=0.419\n",
      "#180 loss=0.090 test_loss=0.069 accuracy=0.979 time=0.419\n",
      "#190 loss=0.116 test_loss=0.124 accuracy=0.963 time=0.419\n",
      "#200 loss=0.082 test_loss=0.068 accuracy=0.979 time=0.422\n",
      "#210 loss=0.081 test_loss=0.063 accuracy=0.980 time=0.419\n",
      "#220 loss=0.081 test_loss=0.062 accuracy=0.980 time=0.420\n",
      "#230 loss=0.086 test_loss=0.067 accuracy=0.980 time=0.419\n",
      "#240 loss=0.083 test_loss=0.061 accuracy=0.981 time=0.420\n",
      "#250 loss=0.082 test_loss=0.058 accuracy=0.982 time=0.419\n",
      "#260 loss=0.081 test_loss=0.056 accuracy=0.982 time=0.419\n",
      "#270 loss=0.100 test_loss=0.082 accuracy=0.975 time=0.419\n",
      "#280 loss=0.080 test_loss=0.057 accuracy=0.982 time=0.419\n",
      "#290 loss=0.078 test_loss=0.055 accuracy=0.983 time=0.419\n",
      "#300 loss=0.078 test_loss=0.053 accuracy=0.983 time=0.419\n",
      "#310 loss=0.082 test_loss=0.054 accuracy=0.983 time=0.419\n",
      "#320 loss=0.075 test_loss=0.056 accuracy=0.983 time=0.419\n",
      "#330 loss=0.072 test_loss=0.051 accuracy=0.984 time=0.419\n",
      "#340 loss=0.073 test_loss=0.050 accuracy=0.985 time=0.421\n",
      "#350 loss=0.120 test_loss=0.106 accuracy=0.964 time=0.419\n",
      "#360 loss=0.076 test_loss=0.055 accuracy=0.982 time=0.419\n",
      "#370 loss=0.076 test_loss=0.051 accuracy=0.984 time=0.419\n",
      "#380 loss=0.075 test_loss=0.049 accuracy=0.984 time=0.419\n",
      "#390 loss=0.076 test_loss=0.049 accuracy=0.985 time=0.419\n",
      "#400 loss=0.080 test_loss=0.053 accuracy=0.983 time=0.419\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f36837de0cc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_in=784\n",
    "n_out=10\n",
    "n_hidden=500\n",
    "model = \"lr\" # logistic regression\n",
    "model = \"fc2\" # 2-layer MLP\n",
    "model = \"conv\" # convolution\n",
    "if (model == \"lr\"):\n",
    "    net = nn.Sequential(nn.Linear(n_in,n_out))\n",
    "elif(model == \"fc2\"):\n",
    "    net = nn.Sequential(nn.Linear(n_in,n_hidden),nn.Sigmoid(),nn.Linear(n_hidden,n_out))\n",
    "elif(model == \"conv\"):\n",
    "    net = nn.Sequential(nn.Conv2d(1,20,5),nn.ReLU(),nn.MaxPool2d(2),nn.Conv2d(20,50,5),nn.ReLU(),nn.MaxPool2d(2))\n",
    "    net2 = nn.Sequential(nn.Linear(4*4*50,n_hidden),nn.ReLU(),nn.Linear(n_hidden,n_out))\n",
    "else:\n",
    "    print(\"Wrong model\")\n",
    "net = net.to(device)\n",
    "net2 = net2.to(device)\n",
    "optimizer = torch.optim.Adam(list(net.parameters()), lr=0.001, betas=(0.9, 0.999))\n",
    "print_bin = epochs/100\n",
    "for epoch in range(1, epochs + 1):\n",
    "    optimizer.zero_grad()\n",
    "    t1=time.time()\n",
    "    for batch_idx, (data, target) in enumerate(train_data):\n",
    "        if(model == 'conv'):\n",
    "            output=net(data).view(output.shape[0],-1)\n",
    "            output = net2(output)\n",
    "        else:\n",
    "            output=net(data.view(data.shape[0],-1))\n",
    "\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_data:\n",
    "        if(model == 'conv'):\n",
    "            output=net(data).view(output.shape[0],-1)\n",
    "            output = net2(output)\n",
    "        else:\n",
    "            output=net(data.view(data.shape[0],-1))\n",
    "        output=F.log_softmax(output, dim=1)\n",
    "        test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= 10000\n",
    "    correct /= 10000\n",
    "    if(epoch % print_bin==0):\n",
    "        print(\"#%d loss=%.3f test_loss=%.3f accuracy=%.3f time=%.3f\"%(epoch,loss,test_loss,correct,time.time()-t1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
